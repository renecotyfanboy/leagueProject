{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#whats-in-this-website","title":"What's in this website?","text":"<p>This website is a collection of things I have done around data science and League of Legends. At the moment, it is mainly focused on the analysis I did around the (non-)existence of the LoserQ mechanism. As we would say in France, this work is \"fait avec soin par mes petites mains\", but I did my best to get a (near) peer review process by asking scientist friends of mine to proofread it.</p> LoserQ Infographic <p></p>"},{"location":"#who-worked-on-this","title":"Who worked on this ?","text":"<ul> <li>renecotyfanboy, (Simon Dupourqu\u00e9, PhD) : the author of this work. Working on statistical inference for X-ray spectra and for turbulence in galaxy clusters.</li> </ul> <p>The following people are colleagues with whom I was fortunate enough to spend my Ph.D. years, and with whom I continue to work whenever possible. They have graciously given me their time to proofread this work, as reviewers would do for scientific journals.</p> <ul> <li>Xan Astiasarain (PhD) : internal reviewer. Formerly worked on the generation of cosmic rays in the Cygnus region, using the Fermi space telescope gamma-ray observatory.</li> <li>Alexei Molin (PhD student) : internal reviewer. Working around the characterization of future X-ray space observatory and calibrating its possibilities for new science.</li> <li>Erwan Quintin (PhD) : internal reviewer. Working on statistical cross-matching of X-ray catalog to detect formerly unknown transient events.</li> </ul>"},{"location":"api/data/","title":"data","text":""},{"location":"api/data/#leaguedata.data","title":"<code>leaguedata.data</code>","text":""},{"location":"api/data/#leaguedata.data.get_dataset","title":"<code>get_dataset(select_columns=None)</code>","text":"<p>Load the dataset from the Hugging Face Hub and return it as a Polars DataFrame.</p> PARAMETER DESCRIPTION <code>select_columns</code> <p>The columns to select from the dataset. If None, all columns are selected.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>pl.DataFrame: The dataset as a Polars DataFrame.</p> Source code in <code>leaguedata/data.py</code> <pre><code>def get_dataset(select_columns=None) -&gt; pl.DataFrame:\n    \"\"\"\n    Load the dataset from the Hugging Face Hub and return it as a Polars DataFrame.\n\n    Parameters:\n        select_columns (list): The columns to select from the dataset. If None, all columns are selected.\n\n    Returns:\n        pl.DataFrame: The dataset as a Polars DataFrame.\n    \"\"\"\n\n    dataset = load_dataset(\"renecotyfanboy/leagueData\", split=\"train\")\n\n    if select_columns is None:\n        return dataset.to_polars()\n\n    return dataset.select_columns(select_columns).to_polars()\n</code></pre>"},{"location":"api/data/#leaguedata.data.get_history_dict","title":"<code>get_history_dict()</code>","text":"<p>Return a two level dictionary containing the history of all players in the reference sample. Accessed by elo and then by puuid.</p> Source code in <code>leaguedata/data.py</code> <pre><code>def get_history_dict():\n    \"\"\"\n    Return a two level dictionary containing the history of all players in the reference sample.\n    Accessed by elo and then by puuid.\n    \"\"\"\n\n    columns = ['elo', 'puuid', 'gameStartTimestamp', 'is_in_reference_sample', 'win']\n    df = get_dataset(columns)\n    unique_elo = df.filter(is_in_reference_sample=True)['elo'].unique()\n\n    history = {}\n\n    for elo in unique_elo:\n        loc_df = df.filter(elo=elo, is_in_reference_sample=True)\n        history[elo] = {}\n        unique_puuid = loc_df['puuid'].unique()\n\n        for puuid in unique_puuid:\n            loc_history = loc_df.filter(puuid=puuid)\n            history[elo][puuid] = np.asarray(loc_history.sort(by='gameStartTimestamp')['win'])\n\n    return history\n</code></pre>"},{"location":"api/data/#leaguedata.data.get_tier_batch","title":"<code>get_tier_batch()</code>","text":"<p>Return batches of tiers in League of Legends, sorted from the lowest to the highest.</p> Source code in <code>leaguedata/data.py</code> <pre><code>def get_tier_batch() -&gt; list:\n    \"\"\"\n    Return batches of tiers in League of Legends, sorted from the lowest to the highest.\n    \"\"\"\n\n    tier_list = ['IRON', 'BRONZE', 'SILVER', 'GOLD', 'PLATINUM', 'EMERALD', 'DIAMOND', 'MASTER', 'GRANDMASTER',\n                 'CHALLENGER']\n    division_list = ['I', 'II', 'III', 'IV'][::-1]\n\n\n    for tier in tier_list:\n\n        tier_with_sub = []\n\n        if tier not in ['MASTER', 'GRANDMASTER', 'CHALLENGER']:\n            for division in division_list:\n                tier_with_sub.append(f'{tier}_{division}')\n\n            yield tier_with_sub\n\n        else:\n            yield [tier]\n</code></pre>"},{"location":"api/data/#leaguedata.data.get_tier_sorted","title":"<code>get_tier_sorted()</code>","text":"<p>Return a list of all the tiers in League of Legends, sorted from the lowest to the highest.</p> RETURNS DESCRIPTION <code>list</code> <p>The list of all tiers in League of Legends, sorted from the lowest to the highest.</p> <p> TYPE: <code>list</code> </p> Source code in <code>leaguedata/data.py</code> <pre><code>def get_tier_sorted() -&gt; list:\n    \"\"\"\n    Return a list of all the tiers in League of Legends, sorted from the lowest to the highest.\n\n    Returns:\n        list: The list of all tiers in League of Legends, sorted from the lowest to the highest.\n    \"\"\"\n\n    tier_list = ['IRON', 'BRONZE', 'SILVER', 'GOLD', 'PLATINUM', 'EMERALD', 'DIAMOND', 'MASTER', 'GRANDMASTER',\n                 'CHALLENGER']\n    division_list = ['I', 'II', 'III', 'IV'][::-1]\n    tier_with_sub = []\n\n    for tier in tier_list:\n\n        if tier not in ['MASTER', 'GRANDMASTER', 'CHALLENGER']:\n            for division in division_list:\n                tier_with_sub.append(f'{tier}_{division}')\n\n    return tier_with_sub + ['MASTER', 'GRANDMASTER', 'CHALLENGER']\n</code></pre>"},{"location":"api/inference/","title":"inference","text":""},{"location":"api/inference/#leaguedata.inference","title":"<code>leaguedata.inference</code>","text":""},{"location":"api/inference/#leaguedata.inference.numpyro_model","title":"<code>numpyro_model(markov_model, observed_data)</code>","text":"<p>Function that is used as a model in NumPyro to perform inference on the Discrete Markov Chain model.</p> PARAMETER DESCRIPTION <code>markov_model</code> <p>The Discrete Markov Chain model to use.</p> <p> TYPE: <code>DTMCModel</code> </p> <code>observed_data</code> <p>The observed data to use for inference.</p> <p> TYPE: <code>array</code> </p> Source code in <code>leaguedata/inference.py</code> <pre><code>def numpyro_model(markov_model, observed_data):\n    \"\"\"\n    Function that is used as a model in NumPyro to perform inference on the Discrete Markov Chain model.\n\n    Parameters:\n        markov_model (DTMCModel): The Discrete Markov Chain model to use.\n        observed_data (jnp.array): The observed data to use for inference.\n    \"\"\"\n\n    if not markov_model.is_bernoulli:\n        proba = numpyro.sample('proba',\n                               dist.Uniform(low=jnp.zeros(2 ** markov_model.n), high=jnp.ones(2 ** markov_model.n)))\n    else:\n        proba = numpyro.sample('proba', dist.Uniform(low=0, high=1)) * jnp.ones(2 ** markov_model.n)\n\n    transition_matrix = markov_model.build_transition_matrix(proba)\n\n    def transition_fn(_, x):\n        return tfd.Categorical(probs=transition_matrix[x])\n\n    encoded_history = np.apply_along_axis(markov_model.binary_serie_to_categorical, 1, observed_data)\n\n    likelihood_dist = tfd.MarkovChain(\n        initial_state_prior=tfd.Categorical(probs=markov_model.uniform_prior),\n        transition_fn=transition_fn,\n        num_steps=encoded_history.shape[1]\n    )\n\n    numpyro.sample('likelihood', likelihood_dist, obs=encoded_history)\n</code></pre>"},{"location":"api/model/","title":"model","text":""},{"location":"api/model/#leaguedata.model","title":"<code>leaguedata.model</code>","text":""},{"location":"api/model/#leaguedata.model.DTMCModel","title":"<code>DTMCModel</code>","text":"<p>Class used to define a Discrete Time Markov Chain for modelling game history.</p> Source code in <code>leaguedata/model.py</code> <pre><code>class DTMCModel:\n    \"\"\"\n    Class used to define a Discrete Time Markov Chain for modelling game history.\n    \"\"\"\n\n    def __init__(self, n):\n        \"\"\"\n        Build a Discrete Markov Chain model.\n\n        Parameters:\n            n (int): The number of game in memory. If n = 0, the model is a Bernoulli process.\n        \"\"\"\n        self.is_bernoulli = n == 0\n        self.n = max(n, 1)\n\n    @property\n    def ref_table(self):\n        \"\"\"\n        Mapping between binary and categorical representation of states.\n        \"\"\"\n        return bidict({state: i for i, state in enumerate(product([0, 1], repeat=self.n))})\n\n    @property\n    def uniform_prior(self):\n        \"\"\"\n        Define a uniform prior over the states.\n\n        Returns:\n            jnp.array: The uniform prior.\n        \"\"\"\n        return jnp.ones((2 ** self.n,)) / 2 ** self.n\n\n    def get_states(self):\n        \"\"\"\n        Get all the states of the model, which are the combinations of n binary values.\n\n        Returns:\n            list: The list of all states.\n        \"\"\"\n\n        states = []\n\n        for i in range(2 ** self.n):\n            states.append(self.ref_table.inv[i])\n\n        return states\n\n    def build_transition_matrix(self, probs):\n        \"\"\"\n        Build the transition matrix of the model.\n\n        Parameters:\n            probs (jnp.array): The probabilities of winning at each state.\n\n        Returns:\n            jnp.array: The transition matrix.\n        \"\"\"\n\n        transition_matrix = jnp.zeros((2 ** self.n, 2 ** self.n))\n\n        for i in range(2 ** self.n):\n            state_i = self.ref_table.inv[i]\n            i_to_win = self.ref_table[state_i[1:self.n] + (1,)]\n            i_to_lose = self.ref_table[state_i[1:self.n] + (0,)]\n            win_prob = probs.at[i].get()\n            transition_matrix = transition_matrix.at[i, i_to_win].set(win_prob)\n            transition_matrix = transition_matrix.at[i, i_to_lose].set(1 - win_prob)\n\n        return transition_matrix\n\n    def binary_serie_to_categorical(self, serie):\n        \"\"\"\n        Convert a binary representation of states to a categorical representation.\n\n        Parameters:\n            serie (np.array): The binary serie to convert.\n\n        Returns:\n            np.array: The categorical serie.\n        \"\"\"\n\n        new_serie = np.empty((len(serie) - self.n + 1), dtype=int)\n\n        for i in range(len(new_serie)):\n            state = tuple(serie[i:i + self.n])\n            new_serie[i] = self.ref_table[state]\n\n        return new_serie\n\n    def categorical_serie_to_binary(self, serie):\n        \"\"\"\n        Convert a categorical representation of states to a binary representation.\n\n        Parameters:\n            serie (np.array): The categorical serie to convert.\n\n        Returns:\n            np.array: The binary serie.\n        \"\"\"\n\n        new_serie = np.empty((len(serie) + self.n - 1), dtype=int)\n\n        for i in range(len(serie)):\n            new_serie[i:i + self.n] = self.ref_table.inv[int(serie[i])]\n\n        return new_serie\n\n    def build_process(self, steps, probs=None):\n        \"\"\"\n        Build a Markov Chain process with the given number of steps.\n\n        Parameters:\n            steps (int): The number of steps of the process.\n            probs (jnp.array): The probabilities of winning at each state.\n\n        Returns:\n            tfd.MarkovChain: The Markov Chain process.\n        \"\"\"\n\n        if probs is None:\n            transition_matrix = self.build_transition_matrix(jnp.ones(2 ** self.n) * 0.5)\n        else:\n            transition_matrix = self.build_transition_matrix(probs)\n\n        def transition_fn(_, x):\n            return tfd.Categorical(probs=transition_matrix[x])\n\n        return tfd.MarkovChain(\n            initial_state_prior=tfd.Categorical(probs=self.uniform_prior),\n            transition_fn=transition_fn,\n            num_steps=steps\n        )\n\n    def stationary_distribution(self, probs):\n        \"\"\"\n        Compute the stationary distribution of the model.\n\n        Parameters:\n            probs (jnp.array): The probabilities of winning at each state.\n\n        Returns:\n            jnp.array: The stationary distribution.\n        \"\"\"\n\n        transition_matrix = self.build_transition_matrix(probs)\n        eig_val, eig_ref = eig(transition_matrix, left=True, right=False)\n        stat_distribution = eig_ref[:, np.argwhere(np.isclose(eig_val, 1))[0]]\n        return np.abs(stat_distribution / stat_distribution.sum())\n\n    def to_mermaid(self, probs):\n        \"\"\"\n        Convert the model to a Mermaid graph.\n\n        Parameters:\n            probs (jnp.array): The probabilities of winning at each state.\n\n        Returns:\n            str: The Mermaid graph.\n        \"\"\"\n\n        transition_matrix = self.build_transition_matrix(probs)\n        states = self.get_states()\n\n        graph_str = \"graph LR \\n\"\n\n        for i, state_i in enumerate(states):\n            for j, state_j in enumerate(states):\n                prob = transition_matrix[i, j]\n\n                if prob &gt; 0:\n                    line_str = '\\t'\n                    line_str += f'{\"\".join([\"W\" if i else \"L\" for i in state_i])} --&gt; |{int(prob * 100)}%| {\"\".join([\"W\" if i else \"L\" for i in state_j])}'\n                    graph_str += line_str + '\\n'\n\n        return graph_str\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.ref_table","title":"<code>ref_table</code>  <code>property</code>","text":"<p>Mapping between binary and categorical representation of states.</p>"},{"location":"api/model/#leaguedata.model.DTMCModel.uniform_prior","title":"<code>uniform_prior</code>  <code>property</code>","text":"<p>Define a uniform prior over the states.</p> RETURNS DESCRIPTION <p>jnp.array: The uniform prior.</p>"},{"location":"api/model/#leaguedata.model.DTMCModel.__init__","title":"<code>__init__(n)</code>","text":"<p>Build a Discrete Markov Chain model.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of game in memory. If n = 0, the model is a Bernoulli process.</p> <p> TYPE: <code>int</code> </p> Source code in <code>leaguedata/model.py</code> <pre><code>def __init__(self, n):\n    \"\"\"\n    Build a Discrete Markov Chain model.\n\n    Parameters:\n        n (int): The number of game in memory. If n = 0, the model is a Bernoulli process.\n    \"\"\"\n    self.is_bernoulli = n == 0\n    self.n = max(n, 1)\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.binary_serie_to_categorical","title":"<code>binary_serie_to_categorical(serie)</code>","text":"<p>Convert a binary representation of states to a categorical representation.</p> PARAMETER DESCRIPTION <code>serie</code> <p>The binary serie to convert.</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <p>np.array: The categorical serie.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def binary_serie_to_categorical(self, serie):\n    \"\"\"\n    Convert a binary representation of states to a categorical representation.\n\n    Parameters:\n        serie (np.array): The binary serie to convert.\n\n    Returns:\n        np.array: The categorical serie.\n    \"\"\"\n\n    new_serie = np.empty((len(serie) - self.n + 1), dtype=int)\n\n    for i in range(len(new_serie)):\n        state = tuple(serie[i:i + self.n])\n        new_serie[i] = self.ref_table[state]\n\n    return new_serie\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.build_process","title":"<code>build_process(steps, probs=None)</code>","text":"<p>Build a Markov Chain process with the given number of steps.</p> PARAMETER DESCRIPTION <code>steps</code> <p>The number of steps of the process.</p> <p> TYPE: <code>int</code> </p> <code>probs</code> <p>The probabilities of winning at each state.</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <p>tfd.MarkovChain: The Markov Chain process.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def build_process(self, steps, probs=None):\n    \"\"\"\n    Build a Markov Chain process with the given number of steps.\n\n    Parameters:\n        steps (int): The number of steps of the process.\n        probs (jnp.array): The probabilities of winning at each state.\n\n    Returns:\n        tfd.MarkovChain: The Markov Chain process.\n    \"\"\"\n\n    if probs is None:\n        transition_matrix = self.build_transition_matrix(jnp.ones(2 ** self.n) * 0.5)\n    else:\n        transition_matrix = self.build_transition_matrix(probs)\n\n    def transition_fn(_, x):\n        return tfd.Categorical(probs=transition_matrix[x])\n\n    return tfd.MarkovChain(\n        initial_state_prior=tfd.Categorical(probs=self.uniform_prior),\n        transition_fn=transition_fn,\n        num_steps=steps\n    )\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.build_transition_matrix","title":"<code>build_transition_matrix(probs)</code>","text":"<p>Build the transition matrix of the model.</p> PARAMETER DESCRIPTION <code>probs</code> <p>The probabilities of winning at each state.</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <p>jnp.array: The transition matrix.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def build_transition_matrix(self, probs):\n    \"\"\"\n    Build the transition matrix of the model.\n\n    Parameters:\n        probs (jnp.array): The probabilities of winning at each state.\n\n    Returns:\n        jnp.array: The transition matrix.\n    \"\"\"\n\n    transition_matrix = jnp.zeros((2 ** self.n, 2 ** self.n))\n\n    for i in range(2 ** self.n):\n        state_i = self.ref_table.inv[i]\n        i_to_win = self.ref_table[state_i[1:self.n] + (1,)]\n        i_to_lose = self.ref_table[state_i[1:self.n] + (0,)]\n        win_prob = probs.at[i].get()\n        transition_matrix = transition_matrix.at[i, i_to_win].set(win_prob)\n        transition_matrix = transition_matrix.at[i, i_to_lose].set(1 - win_prob)\n\n    return transition_matrix\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.categorical_serie_to_binary","title":"<code>categorical_serie_to_binary(serie)</code>","text":"<p>Convert a categorical representation of states to a binary representation.</p> PARAMETER DESCRIPTION <code>serie</code> <p>The categorical serie to convert.</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <p>np.array: The binary serie.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def categorical_serie_to_binary(self, serie):\n    \"\"\"\n    Convert a categorical representation of states to a binary representation.\n\n    Parameters:\n        serie (np.array): The categorical serie to convert.\n\n    Returns:\n        np.array: The binary serie.\n    \"\"\"\n\n    new_serie = np.empty((len(serie) + self.n - 1), dtype=int)\n\n    for i in range(len(serie)):\n        new_serie[i:i + self.n] = self.ref_table.inv[int(serie[i])]\n\n    return new_serie\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.get_states","title":"<code>get_states()</code>","text":"<p>Get all the states of the model, which are the combinations of n binary values.</p> RETURNS DESCRIPTION <code>list</code> <p>The list of all states.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def get_states(self):\n    \"\"\"\n    Get all the states of the model, which are the combinations of n binary values.\n\n    Returns:\n        list: The list of all states.\n    \"\"\"\n\n    states = []\n\n    for i in range(2 ** self.n):\n        states.append(self.ref_table.inv[i])\n\n    return states\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.stationary_distribution","title":"<code>stationary_distribution(probs)</code>","text":"<p>Compute the stationary distribution of the model.</p> PARAMETER DESCRIPTION <code>probs</code> <p>The probabilities of winning at each state.</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <p>jnp.array: The stationary distribution.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def stationary_distribution(self, probs):\n    \"\"\"\n    Compute the stationary distribution of the model.\n\n    Parameters:\n        probs (jnp.array): The probabilities of winning at each state.\n\n    Returns:\n        jnp.array: The stationary distribution.\n    \"\"\"\n\n    transition_matrix = self.build_transition_matrix(probs)\n    eig_val, eig_ref = eig(transition_matrix, left=True, right=False)\n    stat_distribution = eig_ref[:, np.argwhere(np.isclose(eig_val, 1))[0]]\n    return np.abs(stat_distribution / stat_distribution.sum())\n</code></pre>"},{"location":"api/model/#leaguedata.model.DTMCModel.to_mermaid","title":"<code>to_mermaid(probs)</code>","text":"<p>Convert the model to a Mermaid graph.</p> PARAMETER DESCRIPTION <code>probs</code> <p>The probabilities of winning at each state.</p> <p> TYPE: <code>array</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The Mermaid graph.</p> Source code in <code>leaguedata/model.py</code> <pre><code>def to_mermaid(self, probs):\n    \"\"\"\n    Convert the model to a Mermaid graph.\n\n    Parameters:\n        probs (jnp.array): The probabilities of winning at each state.\n\n    Returns:\n        str: The Mermaid graph.\n    \"\"\"\n\n    transition_matrix = self.build_transition_matrix(probs)\n    states = self.get_states()\n\n    graph_str = \"graph LR \\n\"\n\n    for i, state_i in enumerate(states):\n        for j, state_j in enumerate(states):\n            prob = transition_matrix[i, j]\n\n            if prob &gt; 0:\n                line_str = '\\t'\n                line_str += f'{\"\".join([\"W\" if i else \"L\" for i in state_i])} --&gt; |{int(prob * 100)}%| {\"\".join([\"W\" if i else \"L\" for i in state_j])}'\n                graph_str += line_str + '\\n'\n\n    return graph_str\n</code></pre>"},{"location":"api/model/#leaguedata.model.generate_coinflip_history","title":"<code>generate_coinflip_history(number_of_games=85, number_of_players=200, key=PRNGKey(42))</code>","text":"<p>Generate mock history of players using the coinflip model.</p> PARAMETER DESCRIPTION <code>number_of_games</code> <p>The number of games in the mock history.</p> <p> TYPE: <code>int</code> DEFAULT: <code>85</code> </p> <code>number_of_players</code> <p>The number of players.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>key</code> <p>The key to generate the mock history.</p> <p> TYPE: <code>PRNGKey</code> DEFAULT: <code>PRNGKey(42)</code> </p> Source code in <code>leaguedata/model.py</code> <pre><code>def generate_coinflip_history(number_of_games=85, number_of_players=200, key=PRNGKey(42)):\n    \"\"\"\n    Generate mock history of players using the coinflip model.\n\n    Parameters:\n        number_of_games (int): The number of games in the mock history.\n        number_of_players (int): The number of players.\n        key (PRNGKey): The key to generate the mock history.\n    \"\"\"\n\n    return np.asarray(jax.random.bernoulli(key, 0.5, shape=(number_of_players, number_of_games)))\n</code></pre>"},{"location":"api/model/#leaguedata.model.generate_nasty_loser_q","title":"<code>generate_nasty_loser_q(number_of_games=85, number_of_players=200, key=PRNGKey(42), return_importance=False)</code>","text":"<p>Generate mock history of players using the nasty loserQ model.</p> PARAMETER DESCRIPTION <code>number_of_games</code> <p>The number of games in the mock history.</p> <p> TYPE: <code>int</code> DEFAULT: <code>85</code> </p> <code>number_of_players</code> <p>The number of players.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>key</code> <p>The key to generate the mock history.</p> <p> TYPE: <code>PRNGKey</code> DEFAULT: <code>PRNGKey(42)</code> </p> <code>return_importance</code> <p>Whether to return the importance of the loserQ for each player.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>leaguedata/model.py</code> <pre><code>def generate_nasty_loser_q(number_of_games=85, number_of_players=200, key=PRNGKey(42), return_importance=False):\n    \"\"\"\n    Generate mock history of players using the nasty loserQ model.\n\n    Parameters:\n        number_of_games (int): The number of games in the mock history.\n        number_of_players (int): The number of players.\n        key (PRNGKey): The key to generate the mock history.\n        return_importance (bool): Whether to return the importance of the loserQ for each player.\n    \"\"\"\n    markov = DTMCModel(4)\n    keys = jax.random.split(key, 2)\n\n    importance = dist.Beta(1.2, 10).sample(keys[0], sample_shape=(number_of_players,))\n\n    def single_history(key, importance, number_of_games):\n        probs = jnp.empty((2 ** 4))\n\n        probs_keys = {0.: 0.5 - 0.375 * importance,\n                      0.25: 0.5 - 0.125 * importance,\n                      0.5: 0.5,\n                      0.75: 0.5 + 0.125 * importance,\n                      1.: 0.5 + 0.375 * importance}\n\n        for i, state in enumerate(markov.get_states()):\n            probs = probs.at[i].set(probs_keys[sum(state) / 4])\n\n        return markov.build_process(number_of_games -3, probs=probs).sample(1, seed=key)[0]\n\n    keys = jax.random.split(keys[1], number_of_players)\n    history_categorical = np.asarray(\n        jax.vmap(lambda key, importance: single_history(key, importance, number_of_games)\n                 )(keys, importance))\n\n    history = np.apply_along_axis(markov.categorical_serie_to_binary, 1, history_categorical)\n\n    if return_importance:\n        return history, importance\n\n    return history\n</code></pre>"},{"location":"api/model/#leaguedata.model.generate_obvious_loser_q","title":"<code>generate_obvious_loser_q(number_of_games=85, number_of_players=200, key=PRNGKey(42))</code>","text":"<p>Generate mock history of players using the obvious loserQ model.</p> PARAMETER DESCRIPTION <code>number_of_games</code> <p>The number of games in the mock history.</p> <p> TYPE: <code>int</code> DEFAULT: <code>85</code> </p> <code>number_of_players</code> <p>The number of players.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>key</code> <p>The key to generate the mock history.</p> <p> TYPE: <code>PRNGKey</code> DEFAULT: <code>PRNGKey(42)</code> </p> Source code in <code>leaguedata/model.py</code> <pre><code>def generate_obvious_loser_q(number_of_games=85, number_of_players=200, key=PRNGKey(42)):\n    \"\"\"\n    Generate mock history of players using the obvious loserQ model.\n\n    Parameters:\n        number_of_games (int): The number of games in the mock history.\n        number_of_players (int): The number of players.\n        key (PRNGKey): The key to generate the mock history.\n    \"\"\"\n\n    markov_util_ref = DTMCModel(4)\n\n    probs = jnp.empty((2 ** 4))\n    importance = 0.5\n\n    probs_keys = {\n        0.: 0.5 - 0.375 * importance,\n        0.25: 0.5 - 0.125 * importance,\n        0.5: 0.5,\n        0.75: 0.5 + 0.125 * importance,\n        1.: 0.5 + 0.375 * importance\n    }\n\n    for i, state in enumerate(markov_util_ref.get_states()):\n        probs = probs.at[i].set(probs_keys[sum(state) / 4])\n\n    mock_history_encoded = markov_util_ref.build_process(number_of_games - 3, probs=probs).sample(number_of_players, seed=key)\n    mock_history = np.apply_along_axis(markov_util_ref.categorical_serie_to_binary, 1, mock_history_encoded)\n\n    return mock_history\n</code></pre>"},{"location":"api/plot/","title":"plot","text":""},{"location":"api/plot/#leaguedata.plot","title":"<code>leaguedata.plot</code>","text":""},{"location":"api/plot/#leaguedata.plot.plot_compare_plotly","title":"<code>plot_compare_plotly(comp_df, save_to=None)</code>","text":"<p>Function to plot the comparison of models using Plotly. Equivalent to <code>arviz.plot_compare</code>.</p> PARAMETER DESCRIPTION <code>comp_df</code> <p>The comparison DataFrame.</p> <p> TYPE: <code>DataFrame</code> </p> <code>save_to</code> <p>The path to save the plot.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>leaguedata/plot.py</code> <pre><code>def plot_compare_plotly(comp_df, save_to=None):\n    \"\"\"\n    Function to plot the comparison of models using Plotly. Equivalent to `arviz.plot_compare`.\n\n    Parameters:\n        comp_df (pd.DataFrame): The comparison DataFrame.\n        save_to (str): The path to save the plot.\n    \"\"\"\n\n    comp_df = comp_df.sort_index()\n\n    plot_kwargs = {\n        \"color_ic\": \"black\",\n        \"marker_ic\": \"circle\",\n        \"marker_fc\": \"white\",\n        \"color_dse\": \"grey\",\n        \"marker_dse\": \"triangle-up\",\n        \"ls_min_ic\": \"dash\",\n        \"color_ls_min_ic\": \"grey\"\n    }\n\n    color_criterion = 'rgba(0,176,246,1.)'\n    color_difference = 'rgba(231,107,243,1.)'\n\n    linewidth = 2\n    information_criterion = 'elpd_loo'\n\n    n_models = len(comp_df)\n    yticks_pos = np.arange(n_models)[::-1] * -1.5  # Increased spacing between ticks\n    labels = comp_df.index.tolist()\n\n    # Create the figure\n    fig = go.Figure()\n\n    # Add the ELPD difference error bars\n    diff_df = comp_df[comp_df['rank']&gt;0]\n\n    fig.add_trace(go.Scatter(\n        x=diff_df[information_criterion],\n        y=yticks_pos[[int(x[0]) for x in diff_df.index]] + 0.6,\n        error_x=dict(\n            type='data', \n            array=diff_df['dse'], \n            thickness=linewidth\n        ),\n        mode='markers+text',\n        marker=dict(\n            color=color_difference, \n            symbol=plot_kwargs[\"marker_dse\"], \n            size=10,\n            line=dict(\n                width=linewidth\n            )\n        ),\n        name=\"ELPD difference\"\n    ))\n\n    # Add the ELPD error bars\n    fig.add_trace(go.Scatter(\n        x=comp_df[information_criterion],\n        y=yticks_pos,\n        error_x=dict(\n            type='data', \n            array=comp_df['se'], \n            thickness=linewidth\n        ),\n        mode='markers+text',\n        marker=dict(\n            color=color_criterion, \n            symbol=plot_kwargs[\"marker_ic\"], \n            size=10, \n            line=dict(\n                #color=plot_kwargs[\"marker_fc\"], \n                width=linewidth\n            )\n        ),\n        name=\"ELPD\"\n    ))\n\n    # Add a vertical line\n    fig.add_shape(\n        type=\"line\", \n        x0=comp_df[comp_df['rank']==0][information_criterion].iloc[0], \n        y0=min(yticks_pos) - 1, \n        x1=comp_df[comp_df['rank']==0][information_criterion].iloc[0], \n        y1=max(yticks_pos) + 1,\n        line=dict(color=plot_kwargs[\"color_ls_min_ic\"], width=linewidth, dash=plot_kwargs[\"ls_min_ic\"]),\n    )\n\n    fig.update_xaxes(showgrid=True, minor=dict(showgrid=True))\n    fig.update_yaxes(showgrid=True, minor=dict(showgrid=True))\n    # Update axes properties\n    fig.update_layout(\n        xaxis_title='log(ELPD LOO) [higher is better]',\n        #yaxis_title='Model',\n        yaxis=dict(\n            tickmode='array',\n            tickvals=yticks_pos,\n            ticktext=labels\n        ),\n        margin=dict(l=20, r=20, t=20, b=20),\n        width=400, height=300,\n        yaxis_autorange='reversed',\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=1.02,\n            xanchor=\"right\",\n            x=1\n        )\n    )\n\n    fig.show()\n\n    if save_to is not None:\n        with open(save_to, \"w\") as f:\n            f.write(fig.to_json())\n\n    return fig\n</code></pre>"},{"location":"api/plot/#leaguedata.plot.plot_history","title":"<code>plot_history(matrix, name=None, lane=None, kda=None, start=None, save_to=None)</code>","text":"<p>Function to plot the history of a player in a heatmap.</p> PARAMETER DESCRIPTION <code>matrix</code> <p>The history matrix to plot.</p> <p> TYPE: <code>array</code> </p> <code>name</code> <p>The name of the player.</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> <code>lane</code> <p>The lane of the player.</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> <code>kda</code> <p>The KDA of the player.</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> <code>start</code> <p>The starting item of the player.</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> <code>save_to</code> <p>The path to save the plot.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>leaguedata/plot.py</code> <pre><code>def plot_history(matrix, name=None, lane=None, kda=None, start=None, save_to=None):\n    \"\"\"\n    Function to plot the history of a player in a heatmap.\n\n    Parameters:\n        matrix (np.array): The history matrix to plot.\n        name (np.array): The name of the player.\n        lane (np.array): The lane of the player.\n        kda (np.array): The KDA of the player.\n        start (np.array): The starting item of the player.\n        save_to (str): The path to save the plot.\n    \"\"\"\n\n    name = name if name is not None else np.empty(matrix.shape, dtype=str)\n    lane = lane if lane is not None else np.empty(matrix.shape, dtype=str)\n    kda = kda if kda is not None else np.empty(matrix.shape, dtype=str)\n    start = start if start is not None else np.empty(matrix.shape, dtype=str)\n\n    heatmap = go.Heatmap(\n        z=matrix.astype(float),\n        customdata=np.dstack((name, matrix.astype(bool), lane, kda, start)),\n        hovertemplate='&lt;b&gt;Summoner: %{customdata[0]}&lt;/b&gt;&lt;br&gt;'\n                      'Win: %{customdata[1]}&lt;br&gt;'\n                      'Lane: %{customdata[2]}&lt;br&gt;'\n                      'KDA: %{customdata[3]:.2f} &lt;br&gt;'\n                      'Start: %{customdata[4]}',\n        colorscale=[[0, 'rgba(199, 21, 133, 0.8)'], [1, 'rgba(60, 179, 113, 0.8)']],\n        name=\"History\",\n        showscale=False\n    )\n\n    # Create a Figure and update layout for a cleaner look\n    fig = go.Figure(data=[heatmap])\n    fig.update_layout(  # Hiding x-axis ticks\n        yaxis=go.layout.YAxis(\n            title='Individual players',\n            showticklabels=False\n        ),\n        margin=dict(l=20, r=20, t=20, b=20),\n        width=600, height=800 / 3,\n        xaxis_title=f'History of {matrix.shape[1]} games',\n    )\n\n    # Show plot\n    fig.show()\n\n    if save_to is not None:\n        with open(save_to, \"w\") as f:\n            f.write(fig.to_json())\n\n    return fig\n</code></pre>"},{"location":"dataset/cookbook/","title":"Cookbook","text":"<p>Here I'll provide a few examples of how to use the dataset using <code>polars</code>.</p>"},{"location":"dataset/cookbook/#load-the-dataset","title":"Load the dataset","text":"<p>Load the dataset from <code>huggingface</code> and display all the available columns.</p> <pre><code>from datasets import load_dataset\n\ndf = load_dataset(\"renecotyfanboy/leagueData\", split=\"train\").to_polars()\nprint(df.columns)\n</code></pre>"},{"location":"dataset/cookbook/#find-the-history-of-a-player","title":"Find the history of a player","text":"<pre><code>puuid = 'your_puuid' # (1)!\nhistoric_of_random_player = df.filter(\n    puuid=puuid, is_in_reference_sample=True # (2)!\n    ).sort(by='gameStartTimestamp')\n</code></pre> <ol> <li><code>b3fhGxFuV-hCD3B5Vvj9nrD--8YwlFACxvAIox_sOq2aNUtmkcsmem8NFufjdZd79L49I9spnh7LQg</code> is a valid <code>puuid</code>.</li> <li><code>is_in_reference_sample=True</code> indicates that we only keep the match history collected initially. Sometimes, the player can appear in the others matches, but for history analysis it would include matches that were not initially selected.</li> </ol>"},{"location":"dataset/cookbook/#lowest-number-of-games","title":"Lowest number of games","text":"<p>Remake games were removed from the dataset, so some players don't have 100 games. This is how we get the lowest number of game for a single player, which is 85.</p> <pre><code>from datasets import load_dataset\n\ncolumns = ['elo', 'puuid', 'gameStartTimestamp', 'is_in_reference_sample', 'win']\ndf = load_dataset(\"renecotyfanboy/leagueData\", split=\"train\").select_columns(columns).to_polars()\ndf = df.filter(is_in_reference_sample=True)\n\nnumber_of_games = []\n\nfor puuid in df['puuid'].unique():\n    player = df.filter(puuid=puuid)\n    number_of_games.append(len(player.sort(by='gameStartTimestamp')['win'].to_numpy()))\n\nmin(number_of_games)\n</code></pre>"},{"location":"dataset/cookbook/#history-of-the-gold-iii-players","title":"History of the Gold III players","text":"<p>Display the history of Gold III players in the dataset as an image.</p> <pre><code>import numpy as np \nimport matplotlib.pyplot as plt \nfrom datasets import load_dataset\n\ncolumns = ['elo', 'puuid', 'gameStartTimestamp', 'is_in_reference_sample', 'win']\ndf = load_dataset(\"renecotyfanboy/leagueData\", split=\"train\").select_columns(columns).to_polars()\ndf = df.filter(elo=\"GOLD_III\", is_in_reference_sample=True)\n\nhistory = []\n\nfor puuid in df['puuid'].unique():\n    player = df.filter(puuid=puuid)\n    history.append(player.sort(by='gameStartTimestamp')['win'].to_numpy()[-85:])\n\nplt.matshow(np.asarray(history))\n</code></pre>"},{"location":"dataset/introduction/","title":"Description","text":"<p>This section is dedicated to the dataset used in the project. It is available  on HuggingFace</p>"},{"location":"dataset/introduction/#data-gathering","title":"Data gathering","text":"<p>All the data gathered here comes from the Riot Games API. I used a personal API key and the awesome  <code>pulsefire</code> Python package. </p> <ol> <li>I collected all the SoloQ players in the <code>EUW</code> server in the evening of 9 April 2024. </li> <li>I randomly picked 100 players in each division, with at least 200 games played in the corresponding split. I assume  that \\(\\sim\\) 100 games are enough for the summoner to be near its true rank. (1) So I gathered the last 100 games played in SoloQ for each of these players.</li> <li>I collected the data for each of these games, and added each player's individual statistics provided by the  MATCH-V5 API as an individual row in the dataset.</li> </ol> <ol> <li></li> </ol> <p>The following plot show the winrate of players in each division.</p> <ul> <li><p> Winrate per division </p> </li> </ul>"},{"location":"loserQ/model/","title":"A bit of theory","text":"<p>Sorry I lied, that's a lot of theory.</p> <p>TL;DR</p> <ul> <li>LoserQ should have a measurable effect on players' game histories. </li> <li>As people would lose more immediately after losing, we should see a clear deviation from randomness in the games, which translates into an intrinsic correlation between the games, and overall longer streaks.</li> <li>Such a mechanism can be modelled using DTMC, a mathematical tool that can efficiently model the fact that the outcome of a game depends on the previous games.</li> <li>Finding the best DTMC to describe the match history can be done within the framework of Bayesian inference. Such a framework allows a powerful comparison of models and can be used to find out how much games are correlated. </li> </ul>"},{"location":"loserQ/model/#introduction","title":"Introduction","text":"<p>As mentioned in the previous section, it is not possible for me to directly investigate things like \"teams are unbalanced from the start\", \"Riot matches me with people with low win rates\", which would be the roots of LoserQ. Riot limit the number of calls per minute to their API. If I had to collect their history for every player, then pick the players in their games and collect the history of those players before the game, it would take eons to build a statistically significant dataset. But if, instead of focusing on the way LoserQ would be implemented, I focus on the effect, the need to call the API is reduced by several orders of magnitude.</p> <p>Let's assume that LoserQ is a real thing. This means that for a given player, the games played over time are not fair, even if they reach their true rank. Especially in this kind of streak mechanism, we should see a clear deviation from randomness (which is what would happen if every game was perfectly fair and the probability of winning a game in every game was exactly 50%). This deviation, if it exists, should be measurable as many players claim to experience it. In particular, we could see that, on average, players tend to lose more immediately after losing and win more immediately after winning. In short, we should see that the result of a game depends on the previous games, which means that the results of games are correlated. </p> <p>The existence of LoserQ could be found in many indirect ways in the game history of players. What comes to mind are the following, which are equivalent ways of presenting the same information:</p> <ul> <li>Streak Length Distribution: We should see that the distribution of streak lengths is, on average, skewed towards longer streaks than what we would observe in a random sequence. This is exactly what many players complain about.</li> <li>Auto-correlation : The outcome of a game should depend on the previous games. This can be measured by the auto-correlation of the sequence of games. If the sequence is random, the auto-correlation should be close to 0.</li> <li>Probability of winning after a win/loss : we should see that the probability of winning a game after a win is greater than the expected 50%, and that the probability of losing a game after a loss is greater than 50%. </li> </ul> <p>In my previous analysis I (quickly) examined the first and third points and showed that the data is consistent with what you would expect from random coin flips in terms of streak lengths. The probability of losing after a loss was slightly higher than expected for this Elo (2% higher than expected). This could be a signature of LoserQ, but it is far too low to explain what people claim to experience, i.e. streaks of 10 losses in a row. Don't get me wrong, this doesn't mean that there aren't summoners who lose or win that much in a row, but I think these streaks are as common as what would happen by pure randomness once they reach their true rank. All of these points can be explored by simply looking at the match history of different players, but I will focus on the third point in this section.</p>"},{"location":"loserQ/model/#lets-meet-with-the-data","title":"Let's meet with the data","text":"<p>Your game history can simply be represented as a series of wins and losses. By collecting this sequence using Riot's API, I can easily plot it as a curve, as shown below. Behold, the history of <code>mopz24#EUW</code>:  </p> <ul> <li><p> Single history of a player </p> </li> </ul> <p>As you can see, this looks like a fancy barcode. There's no way to tell if there's anything special going on here with just our eyes, we'll have to use maths to go a little further. But first, a nice thing to do would be to plot the match history for several players within the same division. </p> <ul> <li><p> Multiple history of players </p> </li> </ul> <p>This has now become a QR code, with the horizontal lines representing the history of games for different players. I'll keep this convention in all the graphs I show. The pattern you see is only due to the stochastic nature of the process, which can deviate from what you would expect from pure coin flips if there is an underlying mechanism at work. This visualisation is of no use in itself, but it is very helpful in developing an intuition about what the data looks like and how it should behave. Just so you know, all my analysis will be based on such observations. To be precise, I will use a dataset of </p> \\[ \\underbrace{31}_{\\text{nb of divisions}}  \\times \\underbrace{100}_{\\text{nb of players}}  \\times \\underbrace{85}_{\\text{nb of matches}} = 263500 \\ \\text{games}\\] <p>which is \\(\\sim2.5\\) times larger than the previous dataset, but also much more diverse (100 players per division) and cleanly selected (only highly involved summoners who played at least 200 games in the current split when collected). This dataset is described and available in the dedicated section of the website. Unfortunately, I can't use the whole dataset at once to do a rigorous analysis. This is because there are many more Silver and Gold players in the ladder than Iron or Challenger players. See for example what is on LeagueOfGraph on this topic. Using the full dataset would lead to biases towards the extremes of the true distribution of players, as it contains significantly more Challenger or Iron players than would be obtained by randomly selecting players. To mitigate this effect, I reduce it accordingly to represent the true distribution of players. This reduces the total number of players to \\(\\sim 2100\\) and the number of games to \\(\\sim 178500\\).</p>"},{"location":"loserQ/model/#markov-chains-for-the-win","title":"Markov chains for the win","text":"<p>Warning</p> <p>This section is an introduction to a mathematical model that will be used to investigate the history of games. This is intended to be a high-level overview of the model, and is not necessary to understand the rest of the website. Don't hesitate to skip if you hate maths and equations.</p> <p>The challenge now is to find a model that can capture the intrinsic correlations in these histories in a quantitative way. As you can see below, game histories are sequences of binary random variables \\(X = \\text{Win}\\) if it's a win and \\(X = \\text{Loss}\\) if it's a loss. The simplest way to model this would be to flip a coin at each game and assign a win or loss depending on the outcome. From a probability point of view, this is equivalent to looking at a Bernoulli process. The outcome of each game is purely random and does not depend on the previous games. In this situation, the outcome of game \\(n\\) is a Bernoulli random variable: </p> \\[ P(X_n = \\text{Win}) = p  \\] <p>The probability \\(p\\) should be close to the player's win rate if we want it to properly model the player's history. Here you can see that this may be too simple. The outcome of a game can depend on the previous games. There are many factors (besides considering LoserQ) that can influence the outcome depending on the previous games, such as the player's mood, fatigue, etc. To model this, we should consider a more complex process where the outcome depends on the previous games. This can be achieved using (discrete time) Markov chains (DTMC). DTMC are a very useful tool for describing random processes where the outcome at a given time depends on a finite number of states in the past. Let's illustrate this with a chain that depends only on the previous game. Such a chain can be represented as a graph:</p> <pre><code>graph LR\n    Win --&gt;|20%| Loss\n    Loss --&gt;|20%| Win\n    Win --&gt;|80%| Win\n    Loss --&gt;|80%| Loss</code></pre> <p>The mathematical counterpart of this chain is encapsulated in 2 equations : </p> \\[ \\left\\{ \\begin{array}{l} P(X_n = \\text{Win} | X_{n-1} = \\text{Win}) = 80\\% \\\\ P(X_n = \\text{Loss} | X_{n-1} = \\text{Loss}) = 80\\%  \\end{array} \\right. \\] <p>We only need to set half of the probabilities because they add up to 1. If \\(P(X_n = \\text{Win} | X_{n-1} = \\text{Win}) = 80\\%\\), then \\(P(X_n = \\text{Loss} | X_{n-1} = \\text{Win}) = 20\\%\\). This is a constraint that we need to remember when setting things up. I will often refer to the probability of the outcome of a game depending on the previous game as the transition probabilities. This set of transition probabilities corresponds to a case where you are much more likely to win after a win, and much more likely to lose after a loss. To draw samples from this chain, we start with a random state and then move to the next state with a probability given by the arrows. For example, if we start with a win, we draw the next outcome between a win and a loss with a probability of 80% and 20% respectively. This is a simple example, but we can imagine more complex chains where the outcome of a game depends on the previous two games, or even more.</p> <pre><code>graph LR\n    A[\"Win, Win\"] --&gt;|70%| A\n    A --&gt;|30%| B[\"Win, Loss\"]\n    C[\"Loss, Win\"] --&gt;|50%| A\n    C --&gt;|50%| B\n    B --&gt;|50%| C\n    D --&gt;|30%| C\n    B --&gt;|50%| D\n    D[\"Loss, Loss\"] --&gt;|70%| D\n</code></pre> <p>Here, there are 4 states to consider which are 1: (Win, Win), 2: (Win, Loss), 3: (Loss, Win), 4: (Loss, Loss). This kind of graph is mathematically equivalent to what we would call a transition matrix \\(\\mathcal{P}\\) in Markov chain theory. </p> Note <p>It is fully equivalent to the following matrix </p> \\[ \\mathcal{P} = \\begin{pmatrix} 0.7 &amp; 0.3 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.5 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.3 &amp; 0.7 \\end{pmatrix} \\text{ where } \\mathcal{P}_{ij} = P(X_n = \\text{state j} | X_{n-1} = \\text{state i}) \\] <p>I will also refer to the number of previous games to be considered as the order of the chain, which is 2 for the previous game. Don't mind if I call this the memory size of the chain. For a chain with \\(m\\) state memory we need to define \\(2^m\\) transition probabilities.</p> Nightmare fuel : DTMC with 4 states <p>You can imagine monstrosities or other biblically accurate angels when considering that the outcome of a game depends on more previous played games. As the number of states to consider grows exponentially, the associated graph becomes unreadable. For example, the graph below represents a chain where the outcome of a game depends on the previous 4 games.</p> <pre><code>graph TB\n    LLLL --&gt; |75%| LLLL\n    LLLL --&gt; |25%| LLLW\n    LLLW --&gt; |67%| LLWL\n    LLLW --&gt; |33%| LLWW\n    LLWL --&gt; |67%| LWLL\n    LLWL --&gt; |33%| LWLW\n    LLWW --&gt; |50%| LWWL\n    LLWW --&gt; |50%| LWWW\n    LWLL --&gt; |67%| WLLL\n    LWLL --&gt; |33%| WLLW\n    LWLW --&gt; |50%| WLWL\n    LWLW --&gt; |50%| WLWW\n    LWWL --&gt; |50%| WWLL\n    LWWL --&gt; |50%| WWLW\n    LWWW --&gt; |34%| WWWL\n    LWWW --&gt; |66%| WWWW\n    WLLL --&gt; |67%| LLLL\n    WLLL --&gt; |33%| LLLW\n    WLLW --&gt; |50%| LLWL\n    WLLW --&gt; |50%| LLWW\n    WLWL --&gt; |50%| LWLL\n    WLWL --&gt; |50%| LWLW\n    WLWW --&gt; |34%| LWWL\n    WLWW --&gt; |66%| LWWW\n    WWLL --&gt; |50%| WLLL\n    WWLL --&gt; |50%| WLLW\n    WWLW --&gt; |34%| WLWL\n    WWLW --&gt; |66%| WLWW\n    WWWL --&gt; |34%| WWLL\n    WWWL --&gt; |66%| WWLW\n    WWWW --&gt; |25%| WWWL\n    WWWW --&gt; |75%| WWWW</code></pre> <p>The way I'm going to describe match histories is based on this model. In my opinion it is a very well motivated model for the following reasons: </p> <ul> <li>It is simple to implement and understand. This kind of model is very handy when it comes to interpreting the results (unlike your favourite AI models), and the logic behind it is simple.</li> <li>It is powerful. This model can capture many patterns, and can be used to study the history of games, even when very fancy mechanisms are involved.</li> </ul>"},{"location":"loserQ/model/#how-to-fit-the-model","title":"How to fit the model","text":"<p>What I will be doing in this analysis is what we call statistical inference. This is the process of inferring the best parameterisation \\(\\theta\\) of an arbitrary model from experimental data \\(D\\). This is a very common practice in science, and is used in many fields such as physics, astrophysics, biology, etc. Here \\(\\theta\\) is the value of the \\(2^m\\) transition probabilities of the DTMC of order \\(m\\) we are considering, and \\(D\\) is the data set of hit histories. There are many ways to both define what it means to be the best parameters and to find them according to this criterion. Personally, I am convinced of the superiority of Bayesian inference approaches (1) for this specific task. In this framework, we characterise our knowledge of the parameters of a model using probability distributions. First, we input some prior knowledge about what we expect to get. Then we update our knowledge of the parameters by confronting it with the data. The update of our knowledge is quantified using the Bayes formula:</p> <ol> <li> Relevant XKCD</li> </ol> \\[ P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)} \\] <ul> <li>\\(P(\\theta)\\) is the prior distribution of the parameters. This is our first guess for the values of the parameters. This is the set we will update with the data.</li> <li>\\(P(D | \\theta)\\) is the likelihood of the data given the parameters. This is the probability of observing the data given the parameters. </li> <li>\\(P(\\theta | D)\\) is the posterior distribution of the parameters that we are looking for. This is the probability of the parameters given the data, and when compared to the distribution \\(P(\\theta)\\), it should be much more informative because we have learned new information from the data.</li> </ul> <ul> <li><p> What's Bayesian inference ? </p> <p>     Using the likelihood of the data, the prior distribution \\(P(\\theta)\\) (blue) is updated, resulting in the posterior distribution \\(P(\\theta|D)\\) (red) of the parameters, which provides a more constrained value.     </p></li> </ul> <p>Normally, both the prior and the likelihood of the Bayes equation are easy to calculate. In the case of a DTMC, we want to fit the transition probabilities, which are the parameters of the model. </p> <ul> <li>The prior for each transition probability is set to a uniform distribution between 0 and 1 (the probabilities cannot be outside this range!). This is the least informative prior we can use, and it will not bias the results towards particular values.</li> <li>The likelihood can be calculated analytically by multiplying the transition probabilities for each transition observed.</li> </ul> <p>For your information, the posterior distribution is generally impossible to calculate using mathematics, because of the limitations of doing mathematics with probability distributions. In general, we do not solve for this distribution, but use mathematical inversions to get samples that are distributed according to it. As you know, the most widely used methods use Markov Chain Monte Carlo approaches to solve these kinds of problems, in particular the <code>NUTS</code> sampler<sup>1</sup>. I won't go into much detail about this on this site, as I think it's far too technical to be of interest to everyone.</p>"},{"location":"loserQ/model/#why-do-we-even-bother-with-this","title":"Why do we even bother with this","text":"<p>Evidence-based science relies heavily on statistics. If you want to make a statement about the world, you need to quantify something we often call \"significance\". This is usually done using the well-known 'p-value'. This is generally defined as the probability of observing something happening under a given hypothesis. In some sciences, a result is considered significant if its p-value is below 5% (or above 95%, depending on the convention). In astrophysics we mostly use the <code>Z-score</code>, which carries the same information. A <code>Z-score</code> of \\(2\\sigma\\) corresponds to a 95% <code>p-value</code>, a <code>Z-score</code> of \\(3\\sigma\\) corresponds to a 99.7% <code>p-value</code>, and a <code>Z-score</code> of \\(5\\sigma\\) corresponds to a 99.9999% <code>p-value</code>. To give you an idea, \\(3\\sigma\\) is an acceptable standard in physics, \\(5\\sigma\\) is the common threshold for a discovery in particle physics at CERN, and \\(5.1\\sigma\\) is the significance of the first gravitational wave event detected by LIGO. But even in science, many people are not clear about what the p-value really is. There is a lot of over-interpretation of this quantity, many people still interpret it as the probability that the hypothesis is true, which is really not the case.</p> <p>The Bayesian approach gives us some great keys to interpreting these kinds of results. Rather than calculating <code>p-values</code>, we focus on comparing different models and finding the one that best describes what we actually observe. This can be done by calculating the ELDP-LOO<sup>2</sup><sup>3</sup>, which is a barbaric term that describes the predictive power of a model, assessed by the behaviour of the analysis when some data points are removed from the analysis. Below is a qualitative representation of model fit. The underfitting model will have a low ELDP value as it is poor at reproducing the data. The over-fitting model will be good at reproducing the data, but will be very unstable when we test it on new data points or remove some from the analysis, leading to a low ELDP value as well. The good model will be both stable when we remove some data points and good at predicting new data points, leading to a high ELDP value.</p> UnderfittingOverfittingGood fitTrue science <ul> <li> <p><p> Too simplistic model </p></p> </li> </ul> <ul> <li> <p><p> Too confident model </p></p> </li> </ul> <ul> <li> <p><p> Just right </p></p> </li> </ul> <ul> <li> <p><p> Best fit with uncertainties  Now that's research. </p></p> </li> </ul> <p>This kind of approach will allow you to compare DTMC with different orders and find the one that best describes the data. Even if it is not straightforward and quite complicated for the non-initiated, this is in my opinion the least biased way to approach such problems. I bother to do all this because I want this study to be as good as possible in terms of my academic standards. This is the kind of methodology I would use in my academic publications.</p>"},{"location":"loserQ/model/#summary","title":"Summary","text":"<p>So what I will do is basically find the best model to describe the history of the games I have collected using the Riot API. I chose to use DTMC to model this dataset because it is easy to implement, powerful in describing the dynamics of the games, and easy to interpret. By fitting the model using MCMC methods, I will be able to find the transition probabilities that best describe the dataset for different memory sizes of the DTMC. Finally, by comparing each DTMC model using the ELDP-LOO metric, I will be able to find the best model to describe the history of the games. This will help to find out what happens in your League of Legends games.</p> <ol> <li> <p>The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo, Hoffman &amp; al (2014) \u21a9</p> </li> <li> <p>Bayesian Measures of Model Complexity and Fit, Spiegelhalter &amp; al (2002) \u21a9</p> </li> <li> <p>Using stacking to average Bayesian predictive distributions, Yao &amp; al (2017) \u21a9</p> </li> </ol>"},{"location":"loserQ/motivation/","title":"Why do I even bother?","text":""},{"location":"loserQ/motivation/#whats-loserq","title":"What's LoserQ?","text":"<p>By now, everyone who plays League of Legends has heard of the LoserQ. It is something that is praised by many streamers (at least in France, where I live), and it is surprisingly difficult to find a clear definition that satisfies everyone. Some people even rejected my previous work, saying that I didn't understand what LoserQ was. Here are some quotes from the internet that try to explain what it is:</p> GhostCalib3r on this Reddit postLLander_ on this Reddit postAcrobaticApricot on this Reddit postMattWolfTV on this Reddit postStraight_Rule_535 on this Reddit post <p>Quote</p> <p>[...] it's the tendency to lose 3\u20135 games in a row after winning 3-5 in a row; \"losers queue into winners queue\"  and vice versa. Some people refer to loser's queue as \"forced 50% winrate\". [...]</p> <p>Quote</p> <p>It's when the matchmaking constantly puts you with people that you have a very low chance to win with</p> <p>Quote</p> <p>I think the idea is that there are 5 losers, all in \u201closer\u2019s queue,\u201d who play against 5 winners. So everyone on a  loser\u2019s queue player\u2019s team is also in loser\u2019s queue. </p> <p>Quote</p> <p>Often when people complain about losers queue it is more about the game being determined  from matchmaking aka loading screen. </p> <p>Quote</p> <p>Idk went to masters with 60% wr, all in lobby has 60-70wr. I lose two matches in a row. Only get matched with  teammates ~45%wr and enemies wr is still 60-70. It might not exist but this is sussy </p> <p>So LoserQ is the kind of thing that everyone understands and feels, but no one can really explain or define. Some people share their personal game histories full of winning and losing streaks and complain about being stuck in LoserQ. Some complain about being matched in games they cannot win in the lobby. A lot of people claim that Riot can know the outcome of a single game from the start, and I can't argue with that! If you have a good proxy for the level of all the players, their mental state and so on, it would be easy to create a lobby where one team is expected to slightly underperform compared to the other team. Add to this the extra leverage of auto-filling players, and you can see why some are convinced that Riot is cheating with their matchmaking. </p> <p>But why would Riot do this? Well, the answer is simple, and it is the same for every company: money. The more you play, the more money you are likely to spend on the game. And the more you are likely to spend money on the game, the more money Riot is likely to make. Therefore, it is in their interest to keep you playing, and the best way to do that is to keep you engaged. And what is the best way to keep you engaged? Well, some people think that getting players into a series of winning and losing streaks is a good way to do this. There is a paper that shows that certain patterns of win/loss can increase player engagement<sup>1</sup>. It is called Engagement Optimised Matchmaking (EOMM), and some people claim that it has been implemented in games like APEX-legend, even though the developers say the opposite. </p> <p>In the EOMM paper, they looked at how some patterns of winning and losing streaks were associated with players leaving the game for longer periods of time. They focused on the correlation between players' last 3 games and players not playing for a week, and found that in their sample, 3% of players are likely to quit if they have won their last 3 games, while 5% of players quit after patterns such as win-win-loss or complete losses. These results are quite anti-LoserQ, as triple losses are the pattern that Riot should avoid at all costs to keep players engaged. But I could argue for LoserQ by saying that these results are not directly exportable to LoL.</p>"},{"location":"loserQ/motivation/#riots-take-on-loserq","title":"Riot's take on LoserQ","text":"<p>Riot Games has always been clear on this issue, claiming that there is no LoserQ in League of Legends. Here is the infamous tweet from Rioter Phroxzon.</p> <p>Losers queue doesn't exist We're not intentionally putting bad players on your team to make you lose more.  (Even if we assumed that premise, wouldn't we want to give you good players so you stop losing?) For ranked, we match you on your rating and that's all. If you've won a\u2026</p>\u2014 Matt Leung-Harrison (@RiotPhroxzon)  February 11, 2024 <p>Many Rioters had similar takes on reddit and other sites, but a portion of the community is still convinced this is a lie. Just take a look at the comments on the tweet to get an idea.</p>"},{"location":"loserQ/motivation/#why-did-i-redo-the-analysis","title":"Why did I redo the analysis?","text":"<p>Comment from Matos3001 on my previous post</p> <p>[...] while [you] might understand a lot about balls in the sky, [you] are no statistician. [...] (1)</p> <p>Back in the summer of 2023, I posted some clues on reddit showing that there was probably no LoserQ mechanism in League of Legends. This post was well received and I had a lot of feedback from the community. There was some criticism, the most constructive being that the sample I collected was only Master+ players, which is not necessarily representative of all players. To be honest, this first analysis was a 4Fun thing that I did while I had to write my manuscript with deadlines that were really tight. The whole process for the reddit post took me one day, from data collection to final publication. This was clearly rushed from a scientific point of view and I wanted to do better when I had more time. Now that I'm defending my PhD (and have the free time to do so), I want to provide a more robust analysis, something I wouldn't be ashamed to publish in a scientific journal.</p> <p>The current analysis was started on 9 April 2024, and it took \\(\\sim 2\\) months of spare time here and there to put all the pieces together, get this website up with all the content, and get it reviewed by people. It took that long because I wanted it to be reproducible, well documented and peer reviewed, unlike any other analysis I have seen. You'll find the dataset on HuggingFace, and the code on the associated GitHub repository. Anyone is welcome to reproduce the analysis and to criticise the methodology, results or interpretation. I am open to any discussion and will update this website with the most relevant comments I receive.</p>"},{"location":"loserQ/motivation/#what-can-i-show-or-not","title":"What can I show or not?","text":"<p>Riot may well know the outcome of a match (although I doubt it) and could use this information to match you with people who will make you lose. They could keep people in a loop of winning and losing, which would make them play more and spend more money on the game. They could autofill you when they want to, or use Tencent's almighty AI to know what colour your underwear is. My goal here is to investigate this and see if there is any evidence of such a mechanism in the data. Don't get me wrong:</p> <ol> <li>I cannot show that Riot is matching you with people who are already losing. That is too much recursive calling of the API, and my poor personal API key would take eons to collect what I need. Also, I generally can't predict the outcome of a game with the data I collect through the API (or at least I can't collect enough data in a short time frame to do so). This is something I might take a look at when I play around with the amazing <code>trueskill2</code> algorithm.</li> <li>I cannot write about in-game feel. I don't care if games feel unwinnable or unloseable, only the results in terms of wins and losses matter in this situation. In particular, I cannot prove or disprove that Riot has perfect control over the outcome of games. But if they do, and if they make it deviate from randomness, I could see it in the dataset.</li> <li>I cannot deny that players go on winning or losing streaks, because they do. Especially in the early seasons, when people are ranked too high or too low, they will experience winning and losing streaks, and this is to be expected as the algorithm is not perfect at predicting your level with a low game count, and it may take some time for you to reach your true rank. This is why I focus on players who are supposedly close to their true rank. The dataset contains the last 100 games of players who played at least 200 games in the first split of 2024, from which remakes are removed, resulting in at least 85 games per player.</li> </ol> <p>However, there are a lot of things you can do with the data from match histories. But as I said in the reddit post, I cannot disprove the existence of the LoserQ. The best I can say is \"if it does exist, it either works or it does not\". We'll discuss this a bit more in the conclusion.</p> <ol> <li> <p>EOMM: An Engagement Optimized Matchmaking Framework, Chen &amp; al. (2017) \u21a9</p> </li> </ol>"},{"location":"loserQ/true_data/","title":"Application to true data","text":"<p>TL;DR</p> <ul> <li>When applied to real data, the best model to describe the history of matches is a 1st order DTMC.</li> <li>Matches in the dataset show a weak but statistically significant correlation with the previous game.</li> <li>The best-fitting model is good at describing the observed streak length distribution and autocorrelation.</li> </ul>"},{"location":"loserQ/true_data/#best-fit-model","title":"Best-fit model","text":"<p>It's time to see what we get with our data set of real matches. Let's fit our model and look at the ELDP-LOO comparison plot.</p> <ul> <li> <p><p> Comparison of DTMC Models on True Data </p></p> <p><p>    Comparison of the ELPD-LOO metric for different depths of the DTMC model, along with the difference to the best ELPD-LOO value.  </p></p> </li> </ul> <p>The previous comparison shows that the best model to describe the history we observe is a 1st order DTMC, where the outcome of a game weakly depends on the previous game. In other words, the best way to predict the outcome of a game is to know the outcome of the previous game. This is the most significant dependence we can find using the full dataset. Note that the second and third order models are also compatible, but this is probably because they can also reproduce the first order model, but with a bit of overfitting that reduces the ELPD-LOO metric. This behaviour is similar to what we observed when validating the model on the simulated data. We would expect that a significant LoserQ would induce a much higher order correlation. Let us focus a little more on the first order model. We can plot the associated transition probabilities for this model.</p> <ul> <li> <p><p> Winning after winning </p> </p> </li> <li> <p><p> Losing after winning </p> </p> </li> <li> <p><p> Winning after losing </p> </p> </li> <li> <p><p> Losing after losing </p> </p> </li> </ul> <p>The first thing that stands out is that the parameters are very well constrained around 50% (beware of the x-axes that are between 49% and 51%). The posterior distributions are clearly Gaussian and can be interpreted quite easily. With this dataset you get a probability of winning the next game of \\((50.12 \\pm 0.17) \\%\\) if the previous game was a win and the probability of losing the next game is \\((50.60 \\pm 0.17) \\%\\) if the previous is a loss. There is a small but significant difference between the two, consistent with the hypothesis that the outcome of a game is not completely independent of the previous one. However, these numbers are too low to generate significantly long winning and losing streaks for any player. We will look at these characteristics in more detail in the following sections.</p>"},{"location":"loserQ/true_data/#streak-lengths","title":"Streak lengths","text":"<p>To check that our best-fit model actually describes our data, we can do what is called posterior predictive checks. This is a simple procedure where you generate a lot of data using the best-fit model and compare the distribution of the observables with the real data. This is a good way of checking that the model can reproduce the observed data. One problem with directly comparing the true histories with the model is that it is difficult to compare two random processes directly. Imagine having to compare two series of coin tosses: checking each one to see if the results are the same is nonsense because it is stochastic. However, you can easily compare averaged quantities, such as the mean of the two series, their standard deviation, etc. Below I compare the distribution of streak lengths from the real data set with what we expected to measure using the best-fit model. Below is the distribution of streak lengths in our real dataset of match histories. </p> <ul> <li><p> Streak length distribution </p> <p>     Distribution of the lengths of the streak of wins and losses in the dataset. Note that the y-axis is in log scale.     </p></li> </ul> <p>This kind of looks like the results I got in my previous post, we see that some players are experiencing really long winning or losing streaks (up to 16 wins and 17 losses in a row (1)). That's \\(\\sim 2\\) streaks over \\(10000\\) of a frequency in our dataset.</p> <ol> <li>\u00e7a Fait Beaucoup La Non GIFfrom \u00e7a Fait Beaucoup La Non GIFs </li> </ol> <p>In the next graph I simulate 100 data sets and calculate the streak lengths for each of them to propagate the uncertainties coming from the sample variance and the model. I do the same for the obvious LoserQ model that I used in the validation to show what we would expect in this case.</p> <ul> <li> <p><p> Streak length distribution in the dataset compared to prediction </p> </p> <p> <p> Distribution of the lengths of the streak of wins and losses in the dataset. Note that the y-axis is in log scale. </p></p> </li> </ul> <p>As before, the real data matches the simulated data. This is indicated by the fact that the real data falls between the \\(95\\%\\) confidence interval of the simulated data. This means that the streak lengths we see in the player dataset are compatible with what we expect from the best-fit model. </p>"},{"location":"loserQ/true_data/#auto-correlation","title":"Auto-correlation","text":"<p>The autocorrelation of a sequence is a measure of the similarity between the sequence and a delayed version of itself. This is another type of measure that we can use to check both the correlation within the dataset and the validity of our model. It is also the most visual way to see if the outcome of a game depends on the previous games. It can be calculated using the following formula:</p> \\[ R_X(k) = \\mathbb{E}\\left[ X_i \\times X_{i-k} \\right] \\] <p>This is the measure of the average of the product between two games that are \\(k\\) games apart. If the sequence is random, then the autocorrelation should be close to 0. If the sequence is positively correlated (if you win a game, you have more chances of winning after \\(k\\) others), then the autocorrelation should be close to 1. Conversely, if the sequence is negatively correlated (if you win a game, you have more chances of losing after \\(k\\) others), then the auto-correlation should be close to -1. The following graph shows the auto-correlation of a sequence of games where the outcome of each game depends only on the previous game. </p> <ul> <li><p> Interpretation of auto-correlation </p> </li> </ul> Note <p>The graph of the DTMC model used to draw the previous plot are the following : </p> <ul> <li> <p>Random model <pre><code>graph LR\n    Win --&gt;|50%| Loss\n    Loss --&gt;|50%| Win\n    Win --&gt;|50%| Win\n    Loss --&gt;|50%| Loss</code></pre></p> </li> <li> <p>Correlated model <pre><code>graph LR\n    Win --&gt;|20%| Loss\n    Loss --&gt;|20%| Win\n    Win --&gt;|80%| Win\n    Loss --&gt;|80%| Loss</code></pre></p> </li> <li> <p>Anti-correlated model <pre><code>graph LR\n    Win --&gt;|80%| Loss\n    Loss --&gt;|80%| Win\n    Win --&gt;|20%| Win\n    Loss --&gt;|20%| Loss</code></pre></p> </li> </ul> <p>We compute the auto-correlation for a simulated dataset generated using the best-fit model and compare it to the auto-correlation of the true dataset. The bands represent the 95% spread of the auto-correlation within the simulated dataset, and we overlap samples from the true dataset to show that it is consistent. We also add the auto-correlation of the obvious LoserQ model to show that we would expect higher values up to a lag of 4, which is the order of the underlying DTMC model. </p> <ul> <li><p> Auto-correlation in the dataset compared to prediction </p> </li> </ul> <p>Once again, we see that this quantity is well predicted by our best-fit DTMC model, while a significant deviation from zero should be visible if there were an efficient LoserQ at act. However, the first-order dynamics we have shown are so small that they would have been difficult to detect without the use of DTMC, which provides some additional motivation for this approach. </p>"},{"location":"loserQ/true_data/#conclusion","title":"Conclusion","text":"<p>So have we shown that there is no LoserQ in League of Legend? Finally? For good? Well, no. In fact, it's impossible to prove that unicorns, dragons and good-smelling League players don't exist. This statement also applies to a lot of other things, including LoserQ. I cannot ontologically disprove their existence without looking at the matchmaking source code. However, we can still get a good interpretation from these results. What is shown here is that matches can be modelled very well by a DTMC that only needs the previous game to define its transitions. In short, using only win/loss information, the best way to predict the outcome of a player's match is to look at his last game. He will then have a \\((50.12 \\pm 0.17) \\%\\) chance of winning if his previous game was won, and \\((49.40 \\pm 0.17) \\%\\) if his previous game was lost. This behaviour was confirmed by studying the series lengths or autocorrelation that such dynamics would induce and comparing them with what is observed in the real data.</p> <p>I wouldn't interpret such low values as the result of a process designed to increase player engagement. If this is indeed the case, then Riot's competence is questionable, as the effect of this LoserQ would only be seen once or twice out of several hundred games. In general, it's healthier to come up with simpler, more reasonable interpretations when you're in this kind of situation. Typically, we know that players are more biased after losing<sup>1</sup>, and that players' bias reduces their overall probability of winning<sup>2</sup><sup>3</sup><sup>4</sup>. This elementary explanation isn't necessarily the right one, but because it's simple, it should be preferred until proven otherwise. In the end, even using a more robust methodology and better quality data, we still find the same results as I presented on reddit in 2023.</p> <p>Takeaways</p> <p>Players reduce their win rate by \\((0.60 \\pm 0.17) \\%\\) after a loss and increase it by \\((0.12 \\pm 0.17) \\%\\) after a win. This is a significant departure from randomness, but you can see its effect once or twice for hundreds of players. In this situation, either LoserQ does not exist, or if it does, it is really ineffective. </p> Why people would still believe in LoserQ? <ul> <li>Being ranked too low/high early in the season, leading to large win/loss streaks until the hidden Elo stabilises.</li> <li>Underdamped MMR system, where the official rank would be much faster to move than the hidden elo.</li> <li>Confirmation bias, as many people think they see patterns in the player they are matched against. </li> <li>Coping, as lower ranked players tend to overestimate their own skill and think they are being held back by matchmaking<sup>5</sup>.</li> <li>And many other reasons, I suppose</li> </ul> <ol> <li> <p>Analyzing the changes in the psychological profile of professional League of Legends players during competition, Mateo-Orcajada &amp; al (2022) \u21a9</p> </li> <li> <p>Understanding Tilt in Esports: A Study on Young League of Legends Players, Wu &amp; Lee (2021) \u21a9</p> </li> <li> <p>Exploring Stress in Esports Gaming: Physiological and Data-driven approach on Tilt, Lee (2021) \u21a9</p> </li> <li> <p>Effects of individual toxic behavior on team performance in League of Legends, Monge &amp; O'Brien (2017) \u21a9</p> </li> <li> <p>The psychology of esports players\u2019 ELO Hell: Motivated bias in League of Legends and its impact on players\u2019 overestimation of skill, Aeschbach &amp; al (2023) \u21a9</p> </li> </ol>"},{"location":"loserQ/validation/","title":"Validating the approach","text":"<p>TL;DR</p> <ul> <li>I would like to validate this methodology using mock data to show its ability to identify dynamics in game histories.</li> <li>I propose two types of simulations to do this: an obvious LoserQ mechanism and a nasty LoserQ mechanism.</li> <li>I show that the methodology can accurately recover arbitrary dynamics with a relatively small number of games (\\(\\lesssim 35000\\)) for both the obvious and the nasty mock LoserQ mechanism.</li> </ul>"},{"location":"loserQ/validation/#generating-mock-data","title":"Generating mock data","text":"<p>The previous page was an exhaustive description of the model I chose to describe the history of games in League of Legends. When trying to evaluate things with mathematical models, it is always good to check if the methodology works on simulated data. To do this, I will show that this methodology can recover the parameters of three simulated samples.</p> <ol> <li>A pure coin flip simulation.</li> <li>A simulation where there is an obvious LoserQ mechanism, where your probability of winning is linked to the four previous games you have played. </li> <li>A simulation where there is a nasty LoserQ mechanism, where most players would not see significant patterns, while some would be cursed by long streaks of wins and losses.</li> </ol> <p>The probability of winning the next game is linked to the winning rate of the previous four games (4-order DTMC), the values are highlighted in the table below.</p> Transition probabilities for mock LoserQ mechanisms Win rate of the 4 previous games Probability of winning next game \\(0\\%\\) \\(50\\% - I\\times 37.5\\%\\) \\(25\\%\\) \\(50\\% - I\\times 12.5\\%\\) \\(50\\%\\) \\(50\\%\\) \\(75\\%\\) \\(50\\% + I\\times 12.5\\%\\) \\(100\\%\\) \\(50\\% + I\\times 37.5\\%\\) <ul> <li>\\(I = 0.5\\) for the obvious LoserQ, enabling considerable streaks occurring for everyone.</li> <li>\\(I\\) is drawn from a \\(\\beta\\) random variable \\(\\alpha = 1.2\\) and \\(\\beta=10\\) for the nasty LoserQ, so that most of the players would experience no significant pattern, but some would be cursed by long streaks of wins and losses.</li> </ul> <ul> <li> <p><p> True and simulated history of games for comparison </p></p> Example historyPure coin flipsObvious LoserQNasty LoserQ </li> </ul> Minigame <p>Can you distinguish a true player history compared to simulated ones ? Solution : (1)   History n\u00b01    History n\u00b02    History n\u00b03   </p> <ol> <li> <p>All are simulated (1). The one below is not. It is sampled from summoners in Emerald I.</p> <ol> <li> Prankex Stickerfrom Prankex Stickers </li> </ol> <p> </p> </li> </ol>"},{"location":"loserQ/validation/#practical-implementation","title":"Practical implementation","text":"<p>So, to summarise the theory section, the whole idea would be to recover the underlying dynamics from a given set of game histories. To achieve this, my approach is to determine the best transition probabilities for a given DTMC using MCMC methods. By determining these probabilities for different DTMCs with increasing memory size, we obtain best-fit models for the underlying dynamics. By comparing these models using ELDP-LOO, we can determine the best model to describe the history of the games. </p> <p>In practice, I'll be determining the transition probabilities for DTMC with memory sizes \\(1\\) to \\(6\\). We sample these posterior distributions using the NUTS sampler implemented in the <code>numpyro</code> library. I then compare these models using the comparator implemented in the <code>arviz</code> library. All the code is available on the Github repository, and the API of the helper package I wrote is detailed in the documentation. As a dummy dataset, I generated 85 games for 400 players, using the above methodology for both the obvious and the nasty LoserQ mechanisms. Such a number of games is equivalent to a single division in the dataset, such as Bronze or Gold, in terms of observed data. Most of the computations were done either on the SSP Cloud data, which kindly and freely provides GPUs to Fr*nch academics, or on my personal computer.</p>"},{"location":"loserQ/validation/#assessing-the-performance","title":"Assessing the performance","text":"<p>Let's first look at the comparator plot for the three simulated data sets. We will discuss how to interpret them.</p> Coinflip historyObvious LoserQNasty LoserQ <ul> <li> <p><p> Comparator plot </p></p> </li> </ul> <ul> <li> <p><p> Comparator plot </p></p> </li> </ul> <ul> <li> <p><p> Comparator plot </p></p> </li> </ul> <p>This graph shows the ELDP calculated for different memory sizes, along with the difference to the best ELDP. Plotting these two helps to compare the models and see which are compatible with the best. First we see that the 0-order DTMC is the best model to describe our coinflip dataset, which is great because 0-order DTMC is basically a coinflip too. For the other two, we can see that the higher ELDP models in our comparator are the 4 order models. This is great because this is the memory size I used to generate the fake LoserQs. For the obvious LoserQ we see that the 5-order model is also a contender for first place. Since the true input is a 4-order dynamics, a 5-order dynamics can also reproduce the observed histories, but with lower ELDP since it overfits the data a bit. Same for the 6th order model. For the nasty LoserQ, we see that the 4-order model is also the best, and that the 2-order is the second best. This is quite interesting, since this mechanism is designed to be difficult to detect, and should be disguised as a lower order dynamic for the people who are not violently cursed. </p> <p>Since the apparent LoserQ is a pure DTMC, we can also check the transition probabilities we obtained for the best model and see if they are close to the one I used to run the simulations. I show this in the following plots, where we see that our posterior distributions (in green) agree with the true values (in grey).</p> Transitions (i)Transitions (ii)Transitions (iii)Transitions (iv) <ul> <li> <p><p> (Loss, Loss, Loss, Loss) to Win </p></p> </li> <li> <p><p> (Loss, Loss, Loss, Win) to Win </p></p> </li> <li> <p><p> (Loss, Loss, Win, Loss) to Win </p></p> </li> <li> <p><p> (Loss, Loss, Win, Win) to Win </p></p> </li> </ul> <ul> <li> <p><p> (Loss, Win, Loss, Loss) to Win </p></p> </li> <li> <p><p> (Loss, Win, Lose, Win) to Win </p></p> </li> <li> <p><p> (Loss, Win, Win, Loss) to Win </p></p> </li> <li> <p><p> (Loss, Win, Win, Win) to Win </p></p> </li> </ul> <ul> <li> <p><p> (Win, Loss, Loss, Loss) to Win </p></p> </li> <li> <p><p> (Win, Loss, Loss, Win) to Win </p></p> </li> <li> <p><p> (Win, Loss, Win, Loss) to Win </p></p> </li> <li> <p><p> (Win, Loss, Win, Win) to Win </p></p> </li> </ul> <ul> <li> <p><p> (Win, Win, Loss, Loss) to Win </p></p> </li> <li> <p><p> (Win, Win, Loss, Win) to Win </p></p> </li> <li> <p><p> (Win, Win, Win, Loss) to Win </p></p> </li> <li> <p><p> (Win, Win, Win, Win) to Win </p></p> </li> </ul> <p>We find that we get most of the input parameters back. Some of them are a bit off due to the sampling variance. This is because we are working with a finite number of matches, generated by a random process. In this situation, some transitions are (by pure luck) slightly over- or under-represented, which can lead to a slight deviation in the posterior distribution. Adding a larger number of games would reduce this variance, but this is not necessary for the purposes of this project, since we have shown that we can find the good dynamics with just a few games.</p> <p>To conclude on validation, I'd say that this approach works pretty well on dummy data, and only needs \\(\\sim 34 000\\) matches to show that something is happening or not. It can catch random behaviour and obvious or much more subtle mechanisms that would otherwise be difficult to see. When we apply it to the dataset of real matches, which is five times larger, we will be able to find the best way to describe the history of matches with even more confidence. </p>"},{"location":"trueskill2/introduction/","title":"Introduction","text":"<p>I really like the <code>trueskill</code> model to infer the skill of a player, and it is something I would like to explore  further. </p>"}]}